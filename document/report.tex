\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{bigints}
\begin{document}

\title{Report on Numerical Physics' project

\bf{Phase separation of a binary fluid}  
}
\maketitle
\pagebreak

\tableofcontents
\pagebreak

\section{The Cahn-Hilliard equation}
The Cahn-Hilliard equation is a non linear fourth order differential equation which is useful in solving problems such as the phase separation of a binary liquid mixture, tumor growth, the thermal induced phase separation etc. In our attempt to model the phase separation of the components of a binary fluid, the aim is to solve for a concentration variable $c(\mathbf{x}, t)$ that describes the spatial distribution of the two phases. A main advantage of solving the Cahn-Hilliard equation is that the border between the phases is continuous and does not have to be defined precisely.

We will here give hints on how to derive the C-H equation from the laws of thermodynamics.

First we consider a system of a mole of a binary solid system of molecules A and B, and we calculate its Helmholtz free energy density $F=E-TS$.

We define c as the proportion of B-type molecules : $c=N_B/(N_A+N_B)$

The mixing of the particules from both phases creates a difference in entropy $\Delta S=kln\frac{W}{W_0}$, where $W$ is the number of ways to order $N_A$ molecules of type A and $N_B$ of type B once they have been mixed together. Basic calculations lead to $\Delta S=-R[(1-c)ln(1-c)+c  ln c]$, where R is the universal gas constant.\\

To derive the internal energy change $\Delta E$ due to mixing, we now consider the energies of the different types of bonds (A-A, B-B A-B). 
We have: $E=P_{AA}\epsilon_{AA}+P_{AB}\epsilon_{AB}+P_{BB}\epsilon_{BB}$, with $P_{ij}$ the number of ij bonds and $\epsilon_{ij}$ the energy of the bond. Making the hypothesis that every molecule has $z$ neighbors, we conclude that $N_Az=P_{AB}+2P_{AA}$, and a similar equation for B.

We finally have to find $P_{AB}$. To do so, an argument is that there are $N_az/2$ bonds (Avogadros number) in one mole of the solid, and that each bond has a probability $2c(1-c)$ of being an AB bond (each site is occupied by A (B) with probability 1-c (c)), so that the number of bonds AB is finally $N_azc(1-c)$. All in all, the inernal energy due to the mixing is $\Delta E=\Omega c(1-c)$ (where we don't consider the terms not depending on c), with $\Omega$ a constant, here equal to $zN_a(\epsilon_{AB}-0.5(\epsilon_{AA}+\epsilon_{BB}))$, which can be positive or negative.\\

In the end, the molar Helmholtz free energy reads: $\Delta F=\Omega c(1-c)+RT[(1-c)ln(1-c)+c  ln c]$
This function can be plotted for different temperatures:
\begin{figure}[h!]
\includegraphics[scale=0.7]{Capture}
\caption{Molecular free energy as a function of c for different Temperatures}
\end{figure}

What we notice is that as long as T is greater than $T_{lim}$, the long terme solution to the problem will be homogeneous with concentration 0.5 everywhere, as this configuration will lower the energy of the system. But something more interesting happens when we lower the temperature: the homogeneous solution sponteanously evolves towards a binary fluid with two phases of different c.\\

The next step is to compute the total free energy of the volume of non-uniform concentration: $e(c)=N_V\int_{\Omega}f d\mathbf{x}$ with $N_V$ the density of molecules and $f(c,\nabla c, \nabla^2 c,...)$ the free energy of a non uniform area. Some considerations of Taylor expansions, symmetry, border conditions and calculus lead to the result: $e(c)=\int_{\Omega}[F(c)+\frac{\epsilon^2}{2}|\nabla c|^2]d\mathbf{x}$

To conclude, we introduce the chemical potential $\mu=\frac{\delta e}{\delta c}$. The net flux of components B $I=-M\nabla \mu$ satisfies the continuity equation $\frac{\partial c}{\partial t}=-\nabla I$, and finally gives us the Cahn-Hilliard equation: $$\frac{\partial c}{\partial t}=\Delta(F'(c)-\epsilon^2\Delta c)$$\\


As solving the CH equation explicitly is not possible, numerical simulations are essential to understand long term behaviours of the solutions of the equation. Several methods are possible to solve this equation, we will here focus on spectral methods. 

The first thing we will do is to discretize the time in time steps of $dt$. 

We will now consider the C-H equation:$\frac{\partial \phi}{\partial t}=\Delta(F'(\phi)-\epsilon^2\Delta \phi)$, with $F(\phi)=0.25(\phi^2-1)^2$. Several schemes are possible to recursively solve for $\phi(kdt)$.\\

\textbf{Semi implicit Euler scheme}(Scheme 2)
The fourth order term will be treated implicitly and the others explicitly. Taking the Fourier transform of the equation and applying this scheme gives: 

$$(1+dt\epsilon^2|\textbf{k}|^4)\tilde{\phi}^{n+1}(\textbf{k})=\tilde{\phi}^n(\textbf{k})-dt\textbf{k}^2\tilde{F'(\phi^n)}(\textbf{k})$$

\textbf{Linearly stabilized splitting scheme}(Scheme 5)
In this scheme the $-\phi$ in F' is split in $-3\phi$ that are treated explicitly and $2\phi$ that are treated implicitly. The rest doesn't differ from the semi implicit scheme. This gives:
$$(1+dt(\epsilon^2|\textbf{k}|^4-2\textbf{k}^2))\tilde{\phi}^{n+1}(\textbf{k})=\tilde{\phi}^n(\textbf{k})+dt(\textbf{k}^2\tilde{{\phi^n}}(\textbf{k})^3-3\textbf{k}^2\tilde{\phi^n}(\textbf{k}))$$



\section{Spectral solver}
This section details the code {\bf implementation}.
For its actual documentation use the \verb`help()` method in a Python interpreter. 
\verb`help()` is designed to be used interactively and has a simple usage.
In Jupyter pressing \verb`Shift+Tab` shows a popup with the head of the documentation.
You can also directly read the docstrings and comments in the code itself.

%TODO: Decide the best between these words lol
%The design/layout/structure/template/organization of the code is simple.
The design of the code is very simple.
A class representing the problem is implemented (\verb`FFTspectral` or \verb`DCTspectral`),
with attributes, or fields, storing the problem's information \verb`(Lx, Nx, epsilon)`.
None of the attributes are left constant, so everything can be played with mid-run.
This obviously may cause bugs, but the freedom of intervention was preferred in our implementation.
One obvious field that is meant to be updated on-the-fly,
not left static as initial conditions,
is the one representing our desired order parameter $\phi$,
a function of space,
which we aim to determine over time.
It is stored in the field \verb`u` of the object.
The \verb`t` field is used to store and keep track of the elapsed time since the object's initialization.
In the end we can see the data on
$\phi(\mathcal{x}, t)$
and
$\hat{\phi(\mathcal{k}, t)}$
as a snapshot on a given instant in time containing only their spatial/spectral dependency.
The data object can be evolved in time with methods detailed further on in this section.

In the initialization we also create two arrays with a discretized representation of the two axis of the space.
We call it the spatial grid, with \verb`Nx, Ny` equally spaced values in each axis.
The same values \verb`Nx, Ny` are used in its dual, the spectral domain.
The numerical representation of the spectral domain is detailed later.
Instead of leaving these representations as actual grids we separated them in their two component axes, \verb`x` and \verb`y` for example.
Implementing a grid would probably make it clearer, but separating it in two axes was easier for debugging and also for initial development.

The contructor or initializer of the class simply takes all the system's information as input and stores it in the fields of a new object of the class.
A good way to visualize it is that each time we initialize an object of this class a new problem system is created.

The temporal evolution of the system is thus done by updating the data object (its \verb`u` and \verb`u_hat` fields, mainly) dynamically.
The spectral classes provide an evolution method to evolve the data object in time through time steps.
As such, time stepping methods are at the heart of the evolution method.
The stepping method is also set as a field of the object, the \verb`step` field.
It can be reassigned even mid-run.
This is useful for using different stepping methods on different scales of time, since their performance varias between them.


Now for the temporal stepping method implementation.
It is our main 'evolution problem' solver.
It operates on $\hat{\phi}$, the Fourier Transform of $\phi$.
The complicated part, which cost us time,
was the understanding of the numerical representation of the discretized wavelength domain,
which is different for the FFT and DCT algorithms.
Before we enter in further details on this discussion, remember:
we are originally working in a spatial domain,
but we use a Fourier Transform to take it to the wavelength domain.
The \verb`freq` terms in the various Python FFT algorithms allude to frequencies, but our input data is not temporal.
It is spatial.
If you look into it, the help page on the fftfreq method,
on both the scipy and numpy packages,
provides no information on why it is arranged like it is (Positive values first, negatives later).
As for the DCT's range of used frequencies, the scipy.fft.dctn help page is quite easy to understand.
For the type-2 DCT on n-dimensional space the frequency domain is represented as an array of shape \verb`s`,
a simple grid of equally spaced points starting from 0.
The shape \verb`s` defaults to that of the original domain.
We have the option to discretize the spectral domain differently.
The negative values in the frequency grid for the FFT are left latter for a simple reason: they are seldom used.
This is due to redundancy in the transformation of real-valued functions.
The negatives represent complex conjugate terms
$(e^{-2 \pi \lambda t})^* = e^{-2 \pi (-\lambda) t}$.
Since our $\phi$, or \verb`u` field, is real-valued they are needless.
So it actually makes sense to leave these values in the latter half.
For most use cases, only the first half is essential.

\section{Numerical analysis}

1. Conservation of the total relative concentration $\Phi$

We define $\Phi$ as the integral of our order parameter over all the available space $\Omega$, that is,
$$
\Phi(t) = \int_{\Omega} \phi(\mathbf{x},t)dS
$$

Just like the total number of particles, $\Phi$ is conserved.
To demonstrate so, we start by reminding ourselves that $\phi(\mathbf{x},t) = c_A(\mathbf{x},t)-c_B(\mathbf{x},t)$.
Whence
$$
\Phi(t) = N_A - N_B
$$

$\Phi$ is thus conserved over time, since $N_A$ and $N_B$ also are.
As a side-note:
$$
N = N_A + N_B = \int_{\Omega} dS = \mu(\Omega),
$$
which is the measure of the area of $\Omega$, which has 0 error for a fixed-size grid.

Since $2N_A = \Phi+1$, $\Phi$ is conserved if and only if $N_A$ and $N_B$ also are.

Finally, we define as an error metric the change in this quantity.
Numerically, we just calculate the sum of the relative concentration over the whole spatial grid.
We define
\[ \mathcal{E}(t) = |\Phi(t) - \Phi_0| \]
We take the logarithm since the scales are very small and note that for our tests $log_{2}\mathcal{E}(t) \approx -58$.
Which is on the order of the precision of the \verb`float64` type we used ($2^{-53}$).



\section{Understanding the free energy density}
About the function $f(\phi) = F(\phi) + \frac{\epsilon^2}{2}|\nabla{\phi}|^2$ (density of free energy):

The second term on $\phi$ describes the "gradient energy", or how the local variability in the concentration increases the free energy.
So as to decrease the free energy, the system normally increases the entropy through mixing, this is accounted for in the first term $F(\phi)$.
But the second term is responsible for local regularity - locally, it favors regions of less variant concentration.
That is, it accounts for the decrease in the free energy due to the homogeneity in the system's composition, modelling spinodal decomposition in a simple way.
(Higher homogeneity = Smaller local variation in composition = Smaller magnitude of concentration gradient = lower free energy).

So $\epsilon$ could be visualized as being responsible for overall homogeneity.
For higher $\epsilon$ values, to minimize the Helmholtz free energy, the system tends to make the gradient smaller overall, creating less but larger "islands of constancy" pure in each phase.
This can also be seen as less total "transition regions" or "surfaces of variation" between the 2 main phases.
So after long time steps we would expect spherical/circular boundaries between the 2 main phases to form (minimizing the transition surface).
Relatively high $\epsilon$ values give rise to very weak phase separation, resulting in a miscible, homogenous, monophasic fluid.
\end{document}
